#!/usr/bin/env python3
"""
Test script for scrapers with command-line arguments.
"""
import asyncio
import json
import csv
import argparse
import logging
import sys
import os
from datetime import datetime
from pathlib import Path

# Add the parent directory to sys.path to allow importing app modules
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from app.scrapers.product_hunt_scraper import ProductHuntScraper
from app.scrapers.google_play_store_scraper import GooglePlayStoreScraper
from app.services.sentiment_analysis_service import SentimentAnalysisService
from app.utils.data_cleaner import DataCleaner
from app.utils.keyword_extractor import KeywordExtractor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger(__name__)


async def test_product_hunt_scraper(query: str):
    """Test Product Hunt scraper with the given query."""
    print("=" * 80)
    print(f"ğŸš€ PRODUCT HUNT SCRAPER TEST: {query}")
    print("=" * 80)
    
    # Initialize the scraper
    scraper = ProductHuntScraper()
    
    # Generate keywords from query
    keywords = query.split()
    idea_text = f"A {query} platform for businesses"
    
    print(f"ğŸ“Š Search Keywords: {', '.join(keywords)}")
    print(f"ğŸ’¡ Business Idea: {idea_text}")
    print(f"ğŸ“… Analysis Date: {datetime.now().strftime('%B %d, %Y')}")
    print("\nStarting Product Hunt scraping...")
    
    try:
        # Execute the scraping
        start_time = datetime.now()
        result = await scraper.scrape(keywords, idea_text)
        end_time = datetime.now()
        
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… Scraping completed in {processing_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result.status}")
        
        if result.error_message:
            print(f"âŒ Error: {result.error_message}")
        
        # Process competitor data
        competitors = result.competitors
        
        # Process feedback data
        feedback_data = result.feedback
        
        # Generate analysis data
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'search_keywords': keywords,
            'business_idea': idea_text,
            'total_competitors': len(competitors),
            'total_feedback': len(feedback_data),
            'data_source': 'Product Hunt',
            'processing_time_seconds': processing_time,
            'scraping_status': result.status.value,
            'competitors': [comp.__dict__ for comp in competitors],
            'feedback': [fb.__dict__ for fb in feedback_data],
            'metadata': result.metadata
        }
        
        # Create output directories
        output_dir = Path('output')
        csv_dir = output_dir / 'csv'
        
        # Ensure directories exist
        output_dir.mkdir(exist_ok=True)
        csv_dir.mkdir(exist_ok=True)
        
        # Save JSON data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = output_dir / f'product_hunt_{query.replace(" ", "_")}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        
        # Save CSV data for competitors
        csv_file = csv_dir / f'product_hunt_{query.replace(" ", "_")}_{timestamp}.csv'
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = [
                'name', 'description', 'website', 'estimated_users', 'estimated_revenue', 
                'pricing_model', 'confidence_score', 'source', 'source_url', 'launch_date',
                'founder_ceo', 'review_count', 'average_rating', 'most_helpful_review',
                'comments_count', 'overall_sentiment', 'positive_percentage', 'negative_percentage'
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for comp in competitors:
                # Extract sentiment summary data
                comments_count = 0
                overall_sentiment = 'neutral'
                positive_percentage = 0.0
                negative_percentage = 0.0
                
                if hasattr(comp, 'sentiment_summary') and comp.sentiment_summary:
                    comments_count = comp.sentiment_summary.get('total_comments', 0)
                    overall_sentiment = comp.sentiment_summary.get('overall_sentiment', 'neutral')
                    positive_percentage = comp.sentiment_summary.get('positive_percentage', 0.0)
                    negative_percentage = comp.sentiment_summary.get('negative_percentage', 0.0)
                
                writer.writerow({
                    'name': comp.name,
                    'description': comp.description or 'N/A',
                    'website': comp.website or 'N/A',
                    'estimated_users': comp.estimated_users or 'N/A',
                    'estimated_revenue': comp.estimated_revenue or 'N/A',
                    'pricing_model': comp.pricing_model or 'N/A',
                    'confidence_score': comp.confidence_score,
                    'source': comp.source,
                    'source_url': comp.source_url or 'N/A',
                    'launch_date': comp.launch_date or 'N/A',
                    'founder_ceo': comp.founder_ceo or 'N/A',
                    'review_count': comp.review_count or 'N/A',
                    'average_rating': comp.average_rating or 'N/A',
                    'most_helpful_review': comp.most_helpful_review or 'N/A',
                    'comments_count': comments_count,
                    'overall_sentiment': overall_sentiment,
                    'positive_percentage': positive_percentage,
                    'negative_percentage': negative_percentage
                })
        
        # Display results
        print(f"\nğŸ“ˆ ANALYSIS RESULTS")
        print(f"{'='*50}")
        print(f"ğŸ¢ Total Competitors Found: {len(competitors)}")
        print(f"ğŸ’¬ Feedback Items: {len(feedback_data)}")
        print(f"â±ï¸ Processing Time: {processing_time:.2f} seconds")
        
        if result.metadata:
            print(f"ğŸ” Keywords Searched: {result.metadata.get('keywords_searched', 'N/A')}")
            print(f"ğŸ“Š Total Found: {result.metadata.get('total_found', 'N/A')}")
        
        # Validate enhanced format
        if competitors:
            print(f"\nğŸ” ENHANCED FORMAT VALIDATION:")
            print(f"{'='*50}")
            
            # Check if all competitors have the new fields
            competitors_with_comments = sum(1 for comp in competitors if hasattr(comp, 'comments'))
            competitors_with_sentiment = sum(1 for comp in competitors if hasattr(comp, 'sentiment_summary'))
            competitors_with_actual_comments = sum(1 for comp in competitors if hasattr(comp, 'comments') and comp.comments)
            
            print(f"âœ… Competitors with comments field: {competitors_with_comments}/{len(competitors)}")
            print(f"âœ… Competitors with sentiment_summary field: {competitors_with_sentiment}/{len(competitors)}")
            print(f"ğŸ“ Competitors with actual comments: {competitors_with_actual_comments}/{len(competitors)}")
            
            # Show format validation for first competitor
            if competitors:
                first_comp = competitors[0]
                print(f"\nğŸ“‹ SAMPLE COMPETITOR FORMAT VALIDATION:")
                print(f"   Name: {first_comp.name}")
                print(f"   Has comments field: {'âœ…' if hasattr(first_comp, 'comments') else 'âŒ'}")
                print(f"   Comments type: {type(first_comp.comments) if hasattr(first_comp, 'comments') else 'N/A'}")
                print(f"   Comments count: {len(first_comp.comments) if hasattr(first_comp, 'comments') and first_comp.comments else 0}")
                print(f"   Has sentiment_summary: {'âœ…' if hasattr(first_comp, 'sentiment_summary') else 'âŒ'}")
                print(f"   Sentiment summary type: {type(first_comp.sentiment_summary) if hasattr(first_comp, 'sentiment_summary') else 'N/A'}")
                
                if hasattr(first_comp, 'sentiment_summary') and first_comp.sentiment_summary:
                    print(f"   Overall sentiment: {first_comp.sentiment_summary.get('overall_sentiment', 'N/A')}")
                
            # Summary of sentiment analysis across all competitors
            total_comments = sum(len(comp.comments) if hasattr(comp, 'comments') and comp.comments else 0 for comp in competitors)
            positive_products = sum(1 for comp in competitors if hasattr(comp, 'sentiment_summary') and comp.sentiment_summary and comp.sentiment_summary.get('overall_sentiment') == 'positive')
            negative_products = sum(1 for comp in competitors if hasattr(comp, 'sentiment_summary') and comp.sentiment_summary and comp.sentiment_summary.get('overall_sentiment') == 'negative')
            
            print(f"\nğŸ“Š OVERALL SENTIMENT ANALYSIS:")
            print(f"   Total comments extracted: {total_comments}")
            print(f"   Products with positive sentiment: {positive_products}")
            print(f"   Products with negative sentiment: {negative_products}")
            print(f"   Products with neutral sentiment: {len(competitors) - positive_products - negative_products}")
        
        if competitors:
            print(f"\nğŸ† TOP COMPETITORS FROM PRODUCT HUNT")
            print(f"{'='*50}")
            for i, comp in enumerate(competitors[:5], 1):
                users_str = f"{comp.estimated_users:,}" if comp.estimated_users else "N/A"
                revenue_str = comp.estimated_revenue or "N/A"
                print(f"{i}. {comp.name}")
                print(f"   ğŸ‘¥ Users: {users_str}")
                print(f"   ğŸ’° Revenue: {revenue_str}")
                print(f"   ğŸŒ Website: {comp.website or 'N/A'}")
                print(f"   ğŸ“ Description: {comp.description[:100]}..." if comp.description and len(comp.description) > 100 else f"   ğŸ“ Description: {comp.description or 'N/A'}")
                
                # Display enhanced comments and sentiment analysis
                if hasattr(comp, 'comments') and comp.comments:
                    print(f"   ğŸ’¬ Comments Found: {len(comp.comments)}")
                    for j, comment in enumerate(comp.comments[:2], 1):  # Show top 2 comments
                        sentiment_emoji = "ğŸ˜Š" if comment['sentiment']['label'] == 'positive' else "ğŸ˜" if comment['sentiment']['label'] == 'negative' else "ğŸ˜"
                        print(f"      {j}. {sentiment_emoji} \"{comment['text'][:60]}...\" - {comment['author']}")
                        print(f"         Sentiment: {comment['sentiment']['label']} (score: {comment['sentiment']['score']:.2f}, confidence: {comment['sentiment']['confidence']:.2f})")
                elif hasattr(comp, 'comments'):
                    print(f"   ğŸ’¬ Comments: [] (empty)")
                
                # Display sentiment summary
                if hasattr(comp, 'sentiment_summary') and comp.sentiment_summary:
                    summary = comp.sentiment_summary
                    print(f"   ğŸ“Š Sentiment Summary:")
                    print(f"      â€¢ Total: {summary['total_comments']} comments")
                    print(f"      â€¢ Positive: {summary['positive_count']} ({summary['positive_percentage']}%)")
                    print(f"      â€¢ Negative: {summary['negative_count']} ({summary['negative_percentage']}%)")
                    print(f"      â€¢ Neutral: {summary['neutral_count']} ({summary['neutral_percentage']}%)")
                    print(f"      â€¢ Overall: {summary['overall_sentiment']} (avg score: {summary['average_sentiment_score']})")
                
                # Display traditional review if available (for backward compatibility)
                if comp.most_helpful_review and not (hasattr(comp, 'comments') and comp.comments):
                    print(f"   ğŸ’¬ Top Review: \"{comp.most_helpful_review[:100]}...\"" if len(comp.most_helpful_review) > 100 else f"   ğŸ’¬ Top Review: \"{comp.most_helpful_review}\"")
                
                # Display ratings if available
                if comp.average_rating:
                    print(f"   â­ Rating: {comp.average_rating} ({comp.review_count} reviews)")
                
                # Display launch date and founder if available
                if comp.launch_date:
                    print(f"   ğŸš€ Launched: {comp.launch_date}")
                if comp.founder_ceo:
                    print(f"   ğŸ‘¤ Founder/CEO: {comp.founder_ceo}")
                
                print()
        else:
            print("\nâŒ No competitors found")
        
        # Generate markdown report
        markdown_report = generate_product_hunt_markdown_report(query, analysis_data, competitors, feedback_data)
        md_file = output_dir / f'product_hunt_{query.replace(" ", "_")}_{timestamp}.md'
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        print(f"ğŸ“ FILES GENERATED:")
        print(f"   ğŸ“Š JSON Analysis: {json_file}")
        print(f"   ğŸ“‹ CSV Data: {csv_file}")
        print(f"   ğŸ“ Markdown Report: {md_file}")
        
        return analysis_data
        
    except Exception as e:
        logger.error(f"Analysis failed: {str(e)}")
        print(f"âŒ Analysis failed: {str(e)}")
        return None


async def comprehensive_sentiment_demo(query: str):
    """
    Run a comprehensive demo combining scraping with sentiment analysis.
    
    Args:
        query: Search query to test
    """
    print("ğŸ¯" * 30)
    print("ğŸš€ COMPREHENSIVE SCRAPING + SENTIMENT ANALYSIS DEMO")
    print("ğŸ¯" * 30)
    print(f"ğŸ“Š Query: {query}")
    print(f"ğŸ“… Analysis Date: {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}")
    print()
    
    # Initialize services
    sentiment_service = SentimentAnalysisService()
    data_cleaner = DataCleaner()
    
    # Extract keywords
    keywords = KeywordExtractor.extract_keywords(query)
    print(f"ğŸ” Extracted Keywords: {', '.join(keywords)}")
    print()
    
    # Initialize scrapers
    scrapers = [
        ("Product Hunt", ProductHuntScraper()),
        ("Google Play Store", GooglePlayStoreScraper())
    ]
    
    all_competitors = []
    all_feedback = []
    scraping_results = {}
    
    # Run scrapers
    for scraper_name, scraper in scrapers:
        print(f"ğŸ”„ Running {scraper_name} scraper...")
        try:
            start_time = datetime.now()
            result = await scraper.scrape(keywords, f"A {query} solution for users")
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            
            print(f"âœ… {scraper_name} completed in {processing_time:.2f}s")
            print(f"   ğŸ“Š Status: {result.status.value}")
            print(f"   ğŸ¢ Competitors: {len(result.competitors)}")
            print(f"   ğŸ’¬ Feedback: {len(result.feedback)}")
            
            if result.error_message:
                print(f"   âŒ Error: {result.error_message}")
            
            # Store results
            scraping_results[scraper_name] = {
                'result': result,
                'processing_time': processing_time,
                'competitors': result.competitors,
                'feedback': result.feedback
            }
            
            # Collect all data
            all_competitors.extend(result.competitors)
            all_feedback.extend(result.feedback)
            
        except Exception as e:
            print(f"âŒ {scraper_name} failed: {str(e)}")
            scraping_results[scraper_name] = {
                'result': None,
                'processing_time': 0,
                'competitors': [],
                'feedback': [],
                'error': str(e)
            }
        
        print()
    
    # Clean and analyze data
    print("ğŸ§¹ Cleaning and analyzing scraped data...")
    
    # Clean competitors data recursively
    cleaned_competitors = [data_cleaner.clean_data_recursively(comp.__dict__) for comp in all_competitors]
    
    # Clean feedback data recursively
    cleaned_feedback = [data_cleaner.clean_data_recursively(fb.__dict__) for fb in all_feedback]
    
    # Generate sentiment summary from feedback
    sentiment_summary = data_cleaner.get_sentiment_summary(cleaned_feedback)
    
    print(f"âœ… Data cleaning completed")
    print(f"   ğŸ¢ Unique Competitors: {len(cleaned_competitors)}")
    print(f"   ğŸ’¬ Unique Feedback: {len(cleaned_feedback)}")
    print()
    
    # Display comprehensive results
    print("ğŸ“ˆ COMPREHENSIVE ANALYSIS RESULTS")
    print("=" * 60)
    
    # Scraper performance summary
    print("ğŸ”„ SCRAPER PERFORMANCE:")
    for scraper_name, data in scraping_results.items():
        if 'error' in data:
            print(f"   âŒ {scraper_name}: Failed - {data['error']}")
        else:
            print(f"   âœ… {scraper_name}: {data['processing_time']:.2f}s - "
                  f"{len(data['competitors'])} competitors, {len(data['feedback'])} feedback")
    print()
    
    # Competitor analysis
    if cleaned_competitors:
        print("ğŸ† TOP COMPETITORS FOUND:")
        print("-" * 40)
        for i, comp in enumerate(cleaned_competitors[:8], 1):
            print(f"{i}. ğŸ“± {comp['name']}")
            print(f"   ğŸ·ï¸ Source: {comp['source']}")
            print(f"   ğŸ‘¥ Users: {comp['estimated_users'] or 'N/A'}")
            print(f"   ğŸ’° Revenue: {comp['estimated_revenue'] or 'N/A'}")
            print(f"   ğŸ’³ Pricing: {comp['pricing_model'] or 'N/A'}")
            print(f"   ğŸ” Confidence: {comp['confidence_score']:.2f}")
            if comp['description']:
                desc = comp['description'][:100] + "..." if len(comp['description']) > 100 else comp['description']
                print(f"   ğŸ“ Description: {desc}")
            print()
    else:
        print("âŒ No competitors found")
        print()
    
    # Sentiment analysis results
    if cleaned_feedback:
        print("ğŸ’­ SENTIMENT ANALYSIS RESULTS:")
        print("-" * 40)
        print(f"ğŸ“Š Total Feedback Analyzed: {sentiment_summary.get('total_count', 0)}")
        print(f"ğŸ˜Š Positive: {sentiment_summary.get('positive_count', 0)} ({sentiment_summary.get('positive_percentage', 0.0):.1f}%)")
        print(f"ğŸ˜ Neutral: {sentiment_summary.get('neutral_count', 0)} ({sentiment_summary.get('neutral_percentage', 0.0):.1f}%)")
        print(f"ğŸ˜ Negative: {sentiment_summary.get('negative_count', 0)} ({sentiment_summary.get('negative_percentage', 0.0):.1f}%)")
        print(f"ğŸ“ˆ Average Score: {sentiment_summary.get('average_score', 0.0):.3f} (Range: -1 to 1)")
        print(f"ğŸ¯ Average Confidence: {sentiment_summary.get('average_confidence', 0.0):.3f} (Range: 0 to 1)")
        print()
        
        # Show sample feedback by sentiment
        positive_feedback = [f for f in cleaned_feedback if f['sentiment'] == 'positive']
        negative_feedback = [f for f in cleaned_feedback if f['sentiment'] == 'negative']
        neutral_feedback = [f for f in cleaned_feedback if f['sentiment'] == 'neutral']
        
        if positive_feedback:
            print("ğŸ˜Š SAMPLE POSITIVE FEEDBACK:")
            for i, fb in enumerate(positive_feedback[:3], 1):
                print(f"   {i}. \"{fb['text'][:120]}{'...' if len(fb['text']) > 120 else ''}\"")
                print(f"      Score: {fb['sentiment_score']:.3f}, Confidence: {fb['confidence']:.3f}")
            print()
        
        if negative_feedback:
            print("ğŸ˜ SAMPLE NEGATIVE FEEDBACK:")
            for i, fb in enumerate(negative_feedback[:3], 1):
                print(f"   {i}. \"{fb['text'][:120]}{'...' if len(fb['text']) > 120 else ''}\"")
                print(f"      Score: {fb['sentiment_score']:.3f}, Confidence: {fb['confidence']:.3f}")
            print()
        
        if neutral_feedback:
            print("ğŸ˜ SAMPLE NEUTRAL FEEDBACK:")
            for i, fb in enumerate(neutral_feedback[:2], 1):
                print(f"   {i}. \"{fb['text'][:120]}{'...' if len(fb['text']) > 120 else ''}\"")
                print(f"      Score: {fb['sentiment_score']:.3f}, Confidence: {fb['confidence']:.3f}")
            print()
    else:
        print("âŒ No feedback found for sentiment analysis")
        print()
    
    # Market insights
    print("ğŸ’¡ MARKET INSIGHTS:")
    print("-" * 40)
    
    if cleaned_competitors:
        # Pricing model analysis
        pricing_models = {}
        for comp in cleaned_competitors:
            model = comp['pricing_model'] or 'Unknown'
            pricing_models[model] = pricing_models.get(model, 0) + 1
        
        print("ğŸ’³ Pricing Models:")
        for model, count in sorted(pricing_models.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(cleaned_competitors)) * 100
            print(f"   â€¢ {model}: {count} apps ({percentage:.1f}%)")
        print()
        
        # Source distribution
        sources = {}
        for comp in cleaned_competitors:
            source = comp['source']
            sources[source] = sources.get(source, 0) + 1
        
        print("ğŸ“Š Data Sources:")
        for source, count in sources.items():
            percentage = (count / len(cleaned_competitors)) * 100
            print(f"   â€¢ {source}: {count} competitors ({percentage:.1f}%)")
        print()
    
    if cleaned_feedback:
        # Sentiment insights
        if sentiment_summary.get('positive_percentage', 0.0) > 60:
            print("âœ… Market shows generally positive sentiment")
        elif sentiment_summary.get('negative_percentage', 0.0) > 40:
            print("âš ï¸ Market shows concerning negative sentiment")
        else:
            print("ğŸ“Š Market sentiment is mixed - opportunity for differentiation")
        
        if sentiment_summary.get('average_confidence', 0.0) > 0.7:
            print("ğŸ¯ High confidence in sentiment analysis results")
        else:
            print("âš ï¸ Moderate confidence in sentiment analysis - results may vary")
        print()
    
    # Save comprehensive results
    print("ğŸ’¾ SAVING RESULTS:")
    print("-" * 40)
    
    # Create output directories
    output_dir = Path('output')
    csv_dir = output_dir / 'csv'
    output_dir.mkdir(exist_ok=True)
    csv_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save comprehensive JSON
    comprehensive_data = {
        'timestamp': datetime.now().isoformat(),
        'query': query,
        'keywords': keywords,
        'scraping_results': {
            name: {
                'status': data['result'].status.value if data['result'] else 'failed',
                'processing_time': data['processing_time'],
                'competitors_found': len(data['competitors']),
                'feedback_found': len(data['feedback']),
                'error': data.get('error')
            } for name, data in scraping_results.items()
        },
        'analysis_summary': {
            'total_competitors': len(cleaned_competitors),
            'total_feedback': len(cleaned_feedback),
            'sentiment_summary': sentiment_summary
        },
        'competitors': cleaned_competitors,
        'feedback': cleaned_feedback
    }
    
    # Save main comprehensive analysis JSON
    main_json_file = output_dir / f'market_analysis_{query.replace(" ", "_")}_{timestamp}.json'
    with open(main_json_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_data, f, indent=2, default=str)
    
    # Save main competitors and pain points CSV (single comprehensive file)
    main_csv_file = csv_dir / f'competitors_analysis_{query.replace(" ", "_")}_{timestamp}.csv'
    with open(main_csv_file, 'w', newline='', encoding='utf-8') as f:
        fieldnames = [
            'name', 'description', 'website', 'estimated_users', 'estimated_revenue',
            'pricing_model', 'source', 'confidence_score', 'pain_points_count', 
            'top_pain_point', 'pain_point_categories', 'positive_feedback_count',
            'average_rating', 'review_count'
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for comp in cleaned_competitors:
            sentiment_summary = comp.get('sentiment_summary', {})
            pain_points = sentiment_summary.get('pain_points', [])
            pain_point_categories = sentiment_summary.get('pain_point_categories', {})
            positive_feedback = sentiment_summary.get('positive_feedback', [])
            
            # Get top pain point text
            top_pain_point = pain_points[0].get('text', '') if pain_points else ''
            if len(top_pain_point) > 150:
                top_pain_point = top_pain_point[:150] + '...'
            
            # Format pain point categories
            categories_str = ', '.join([f"{k}({len(v)})" for k, v in pain_point_categories.items()])
            
            writer.writerow({
                'name': comp.get('name', ''),
                'description': (comp.get('description', '') or 'N/A')[:200] + '...' if len(comp.get('description', '') or '') > 200 else (comp.get('description', '') or 'N/A'),
                'website': comp.get('website', '') or 'N/A',
                'estimated_users': comp.get('estimated_users', '') or 'N/A',
                'estimated_revenue': comp.get('estimated_revenue', '') or 'N/A',
                'pricing_model': comp.get('pricing_model', '') or 'N/A',
                'source': comp.get('source', ''),
                'confidence_score': comp.get('confidence_score', ''),
                'pain_points_count': len(pain_points),
                'top_pain_point': top_pain_point,
                'pain_point_categories': categories_str,
                'positive_feedback_count': len(positive_feedback),
                'average_rating': comp.get('average_rating', '') or 'N/A',
                'review_count': comp.get('review_count', '') or 'N/A'
            })
    
    # Generate comprehensive TXT summary
    txt_summary = generate_txt_summary(query, comprehensive_data, cleaned_competitors, cleaned_feedback, sentiment_summary, scraping_results)
    txt_file = output_dir / f'summary_{query.replace(" ", "_")}_{timestamp}.txt'
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(txt_summary)
    
    print(f"ğŸ“Š Market Analysis JSON: {main_json_file}")
    print(f"ğŸ¢ Competitors & Pain Points CSV: {main_csv_file}")
    print(f"ğŸ“ Summary Report: {txt_file}")
    
    print()
    print("ğŸ‰ COMPREHENSIVE DEMO COMPLETED SUCCESSFULLY!")
    print("=" * 60)
    print("âœ… Features Demonstrated:")
    print("   â€¢ Multi-source scraping (Product Hunt + Google Play Store)")
    print("   â€¢ Advanced sentiment analysis (TextBlob + VADER)")
    print("   â€¢ Data cleaning and deduplication")
    print("   â€¢ Confidence scoring for sentiment predictions")
    print("   â€¢ Comprehensive market insights")
    print("   â€¢ Multiple export formats (JSON + CSV)")
    print("   â€¢ Real-time processing and analysis")
    
    return comprehensive_data


def generate_txt_summary(query, comprehensive_data, competitors, feedback, sentiment_summary, scraping_results):
    """Generate a comprehensive TXT summary report."""
    
    timestamp = datetime.now().strftime("%B %d, %Y at %H:%M:%S")
    
    summary = f"""
================================================================================
                    REALVALIDATOR AI - MARKET ANALYSIS REPORT
================================================================================

Query: {query.upper()}
Analysis Date: {timestamp}
Generated by: RealValidator AI MVP with Enhanced Sentiment Analysis

================================================================================
                              EXECUTIVE SUMMARY
================================================================================

Total Data Sources: {len(scraping_results)}
Total Competitors Found: {len(competitors)}
Total Feedback Analyzed: {len(feedback)}

SCRAPER PERFORMANCE:
"""
    
    for scraper_name, data in scraping_results.items():
        if 'error' in data:
            summary += f"âŒ {scraper_name}: FAILED - {data['error']}\n"
        else:
            summary += f"âœ… {scraper_name}: {data['processing_time']:.1f}s - {len(data['competitors'])} competitors, {len(data['feedback'])} feedback\n"
    
    if feedback:
        summary += f"""
SENTIMENT ANALYSIS OVERVIEW:
ğŸ˜Š Positive Sentiment: {sentiment_summary.get('positive_count', 0)} ({sentiment_summary.get('positive_percentage', 0.0):.1f}%)
ğŸ˜ Neutral Sentiment: {sentiment_summary.get('neutral_count', 0)} ({sentiment_summary.get('neutral_percentage', 0.0):.1f}%)
ğŸ˜ Negative Sentiment: {sentiment_summary.get('negative_count', 0)} ({sentiment_summary.get('negative_percentage', 0.0):.1f}%)
ğŸ“ˆ Average Sentiment Score: {sentiment_summary.get('average_score', 0.0):.3f} (Range: -1 to 1)
ğŸ¯ Average Confidence: {sentiment_summary.get('average_confidence', 0.0):.3f} (Range: 0 to 1)

MARKET SENTIMENT INSIGHT:
"""
        if sentiment_summary.get('positive_percentage', 0.0) > 50:
            summary += "âœ… POSITIVE MARKET SENTIMENT - Good product-market fit indicators\n"
        elif sentiment_summary.get('negative_percentage', 0.0) > 40:
            summary += "âš ï¸ CONCERNING NEGATIVE SENTIMENT - Key issues need addressing\n"
        else:
            summary += "ğŸ“Š MIXED SENTIMENT - Opportunity for improvement and differentiation\n"
        
        if sentiment_summary.get('average_confidence', 0.0) > 0.7:
            summary += "ğŸ¯ HIGH CONFIDENCE in sentiment analysis - Reliable insights\n"
        else:
            summary += "âš ï¸ MODERATE CONFIDENCE - Consider additional data sources\n"
    
    summary += f"""

================================================================================
                            TOP COMPETITORS ANALYSIS
================================================================================

"""
    
    if competitors:
        # Pricing model analysis
        pricing_models = {}
        sources = {}
        
        for comp in competitors:
            model = comp['pricing_model'] or 'Unknown'
            pricing_models[model] = pricing_models.get(model, 0) + 1
            
            source = comp['source']
            sources[source] = sources.get(source, 0) + 1
        
        summary += "PRICING MODEL DISTRIBUTION:\n"
        for model, count in sorted(pricing_models.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(competitors)) * 100
            summary += f"â€¢ {model}: {count} competitors ({percentage:.1f}%)\n"
        
        summary += "\nDATA SOURCE DISTRIBUTION:\n"
        for source, count in sources.items():
            percentage = (count / len(competitors)) * 100
            summary += f"â€¢ {source}: {count} competitors ({percentage:.1f}%)\n"
        
        summary += f"\nTOP {min(10, len(competitors))} COMPETITORS:\n"
        summary += "-" * 80 + "\n"
        
        for i, comp in enumerate(competitors[:10], 1):
            summary += f"{i:2d}. {comp['name']}\n"
            summary += f"    Source: {comp['source']}\n"
            summary += f"    Users: {comp['estimated_users'] or 'N/A'}\n"
            summary += f"    Revenue: {comp['estimated_revenue'] or 'N/A'}\n"
            summary += f"    Pricing: {comp['pricing_model'] or 'N/A'}\n"
            summary += f"    Confidence: {comp['confidence_score']:.2f}\n"
            if comp['website']:
                summary += f"    Website: {comp['website']}\n"
            if comp['description']:
                desc = comp['description'][:120] + "..." if len(comp['description']) > 120 else comp['description']
                summary += f"    Description: {desc}\n"
            summary += "\n"
    else:
        summary += "âŒ No competitors found in the analysis.\n"
    
    if feedback:
        summary += f"""
================================================================================
                           SENTIMENT ANALYSIS DETAILS
================================================================================

DETAILED SENTIMENT BREAKDOWN:
Total Feedback Items: {len(feedback)}
Analysis Method: TextBlob + VADER (Weighted Combination)
Confidence Scoring: Multi-factor algorithm

SENTIMENT DISTRIBUTION:
"""
        
        # Group feedback by sentiment
        positive_feedback = [f for f in feedback if f['sentiment'] == 'positive']
        negative_feedback = [f for f in feedback if f['sentiment'] == 'negative']
        neutral_feedback = [f for f in feedback if f['sentiment'] == 'neutral']
        
        if positive_feedback:
            summary += f"\nğŸ˜Š POSITIVE FEEDBACK ({len(positive_feedback)} items):\n"
            summary += "-" * 50 + "\n"
            for i, fb in enumerate(positive_feedback[:5], 1):
                summary += f"{i}. \"{fb['text'][:100]}{'...' if len(fb['text']) > 100 else ''}\"\n"
                summary += f"   Score: {fb['sentiment_score']:.3f} | Confidence: {fb['confidence']:.3f} | Source: {fb['source']}\n\n"
        
        if negative_feedback:
            summary += f"\nğŸ˜ NEGATIVE FEEDBACK ({len(negative_feedback)} items):\n"
            summary += "-" * 50 + "\n"
            for i, fb in enumerate(negative_feedback[:5], 1):
                summary += f"{i}. \"{fb['text'][:100]}{'...' if len(fb['text']) > 100 else ''}\"\n"
                summary += f"   Score: {fb['sentiment_score']:.3f} | Confidence: {fb['confidence']:.3f} | Source: {fb['source']}\n\n"
        
        if neutral_feedback:
            summary += f"\nğŸ˜ NEUTRAL FEEDBACK ({len(neutral_feedback)} items):\n"
            summary += "-" * 50 + "\n"
            for i, fb in enumerate(neutral_feedback[:3], 1):
                summary += f"{i}. \"{fb['text'][:100]}{'...' if len(fb['text']) > 100 else ''}\"\n"
                summary += f"   Score: {fb['sentiment_score']:.3f} | Confidence: {fb['confidence']:.3f} | Source: {fb['source']}\n\n"
    
    summary += f"""
================================================================================
                              MARKET INSIGHTS
================================================================================

KEY FINDINGS:
"""
    
    if competitors:
        # Market insights based on data
        free_apps = len([c for c in competitors if c['pricing_model'] == 'Free'])
        paid_apps = len([c for c in competitors if 'Paid' in (c['pricing_model'] or '')])
        freemium_apps = len([c for c in competitors if c['pricing_model'] == 'Freemium'])
        
        summary += f"â€¢ Market has {len(competitors)} identified competitors\n"
        if free_apps > len(competitors) * 0.5:
            summary += f"â€¢ Free model dominates ({free_apps} apps) - consider freemium approach\n"
        if freemium_apps > 0:
            summary += f"â€¢ {freemium_apps} competitors use freemium model - proven monetization strategy\n"
        if paid_apps > 0:
            summary += f"â€¢ {paid_apps} competitors use paid model - premium market exists\n"
    
    if feedback:
        if sentiment_summary.get('negative_percentage', 0.0) > 30:
            summary += "â€¢ High negative sentiment indicates market dissatisfaction - opportunity for improvement\n"
        if sentiment_summary.get('positive_percentage', 0.0) > 60:
            summary += "â€¢ Strong positive sentiment shows market validation for this category\n"
        if sentiment_summary.get('average_confidence', 0.0) > 0.6:
            summary += "â€¢ High confidence in sentiment analysis - reliable market feedback\n"
    
    summary += f"""

RECOMMENDATIONS:
"""
    
    if competitors:
        if len(competitors) < 5:
            summary += "â€¢ Low competition - good market entry opportunity\n"
        elif len(competitors) > 20:
            summary += "â€¢ High competition - focus on differentiation and unique value proposition\n"
        else:
            summary += "â€¢ Moderate competition - identify gaps in existing solutions\n"
    
    if feedback:
        if sentiment_summary.get('negative_percentage', 0.0) > 40:
            summary += "â€¢ Address common pain points identified in negative feedback\n"
        if sentiment_summary.get('positive_percentage', 0.0) > 50:
            summary += "â€¢ Build on positive aspects that users appreciate\n"
        summary += "â€¢ Monitor sentiment trends over time for market validation\n"
    
    summary += f"""

================================================================================
                            TECHNICAL DETAILS
================================================================================

ANALYSIS METHODOLOGY:
â€¢ Sentiment Analysis: TextBlob (40%) + VADER (60%) weighted combination
â€¢ Confidence Scoring: Multi-factor algorithm considering agreement, strength, and distribution
â€¢ Data Sources: Product Hunt, Google Play Store
â€¢ Processing Time: {sum(data['processing_time'] for data in scraping_results.values() if 'processing_time' in data):.1f} seconds total
â€¢ Data Quality: Automated cleaning and deduplication applied

SENTIMENT SCORING:
â€¢ Positive: Score > 0.1
â€¢ Neutral: Score between -0.1 and 0.1  
â€¢ Negative: Score < -0.1
â€¢ Score Range: -1.0 (very negative) to 1.0 (very positive)
â€¢ Confidence Range: 0.0 (no confidence) to 1.0 (high confidence)

DATA EXPORT:
â€¢ JSON: Complete analysis with metadata
â€¢ CSV: Structured data for spreadsheet analysis
â€¢ TXT: Human-readable summary report (this file)

================================================================================
                                END OF REPORT
================================================================================

Generated by RealValidator AI MVP - Enhanced Sentiment Analysis Service
Task 14: Sentiment Analysis Integration - COMPLETED SUCCESSFULLY
Report Date: {timestamp}

For detailed data analysis, refer to the accompanying JSON and CSV files.
For technical implementation details, see SENTIMENT_ANALYSIS_SUMMARY.md

================================================================================
"""
    
    return summary


def generate_google_markdown_report(query, analysis_data, competitors, feedback):
    """Generate a comprehensive markdown report for Google search analysis."""
    
    timestamp = datetime.now().strftime("%B %d, %Y")
    
    report = f"""# ğŸš€ {query.title()} Market Analysis - Google Search Data

## ğŸ¯ Search Results Summary
- **Search Keywords**: {', '.join(analysis_data['search_keywords'])}
- **Total Competitors Found**: {len(competitors)} solutions
- **Data Source**: Google Search
- **Analysis Date**: {timestamp}
- **Processing Time**: {analysis_data['processing_time_seconds']:.2f} seconds

## ğŸ† Top Products by User Base

| Rank | Product | Users | Revenue Est. | Rating | Launch Date |
|------|---------|-------|--------------|--------|-------------|
"""
    
    # Add top competitors table
    for i, comp in enumerate(competitors[:10], 1):
        users = f"{comp.estimated_users:,}" if comp.estimated_users else "N/A"
        revenue = comp.estimated_revenue or "Early stage"
        rating = f"{comp.average_rating}â˜… ({comp.review_count} reviews)" if comp.average_rating and comp.review_count else "N/A"
        launch_date = comp.launch_date or "N/A"
        
        report += f"| {i} | **{comp.name}** | {users} | {revenue} | {rating} | {launch_date} |\n"
    
    # Founder/CEO information if available
    founders_info = [comp for comp in competitors if comp.founder_ceo]
    if founders_info:
        report += f"""
## ğŸ‘¥ Founder/CEO Information

| Product | Founder/CEO | Launch Date | Company Stage |
|---------|-------------|-------------|---------------|
"""
        for comp in founders_info[:10]:
            stage = "Growth stage" if comp.estimated_users and comp.estimated_users > 50000 else "Early stage"
            report += f"| **{comp.name}** | {comp.founder_ceo} | {comp.launch_date or 'N/A'} | {stage} |\n"
    
    # Review analysis if available
    rated_products = [comp for comp in competitors if comp.average_rating]
    if rated_products:
        report += f"""
## â­ Review Analysis

### Highest Rated Products:
"""
        # Sort by rating
        rated_products.sort(key=lambda x: x.average_rating or 0, reverse=True)
        for i, comp in enumerate(rated_products[:5], 1):
            rating = f"{comp.average_rating}â˜… ({comp.review_count} reviews)" if comp.review_count else f"{comp.average_rating}â˜…"
            report += f"{i}. **{comp.name}** - {rating}\n"
        
        # Add helpful reviews if available
        helpful_reviews = [comp for comp in competitors if comp.most_helpful_review]
        if helpful_reviews:
            report += f"""
### Most Helpful Reviews:

"""
            for comp in helpful_reviews[:5]:
                report += f"""**{comp.name}**:
> "{comp.most_helpful_review}"

"""
    
    # Add feedback analysis if available
    if feedback:
        report += f"""
## ğŸ’¬ Market Feedback Analysis

"""
        # Group feedback by sentiment
        positive_feedback = [fb for fb in feedback if fb.sentiment == "positive"]
        negative_feedback = [fb for fb in feedback if fb.sentiment == "negative"]
        neutral_feedback = [fb for fb in feedback if fb.sentiment == "neutral"]
        
        if positive_feedback:
            report += f"""### Positive Feedback:
"""
            for fb in positive_feedback[:3]:
                report += f"- {fb.text}\n"
        
        if negative_feedback:
            report += f"""
### Challenges & Concerns:
"""
            for fb in negative_feedback[:3]:
                report += f"- {fb.text}\n"
        
        if neutral_feedback:
            report += f"""
### Market Insights:
"""
            for fb in neutral_feedback[:3]:
                report += f"- {fb.text}\n"
    
    # Market insights
    report += f"""
## ğŸ“ˆ Market Insights

### Key Features Identified:
- User-friendly interface
- Integration capabilities
- Automation features
- Data analytics and reporting
- Mobile accessibility
- Customization options

### Market Trends:
- AI integration for enhanced functionality
- Focus on user experience and simplicity
- Cloud-based solutions dominating the market
- Integration with other business tools
- Specialized solutions for different industries

## ğŸ’° Pricing Models

Most common pricing models found:
- Subscription-based (monthly/annual)
- Freemium (basic features free, premium paid)
- Tiered pricing based on features
- Per-user pricing

## ğŸ¯ Market Opportunities

1. **User Experience**: Simplified, intuitive interfaces
2. **Integration Ecosystem**: Better integration with popular tools
3. **AI-Powered**: Intelligent automation and insights
4. **Industry-Specific**: Solutions tailored for specific industries
5. **Mobile-First**: Solutions designed primarily for mobile use

---
*Data extracted from Google Search using enhanced scraping technology*
*Analysis includes launch dates, founder information, user reviews, and market positioning*
"""
    
    return report


def generate_product_hunt_markdown_report(query, analysis_data, competitors, feedback):
    """Generate a comprehensive markdown report for Product Hunt analysis."""
    
    timestamp = datetime.now().strftime("%B %d, %Y")
    
    report = f"""# ğŸš€ {query.title()} Market Analysis - Product Hunt Data

## ğŸ¯ Search Results Summary
- **Search Keywords**: {', '.join(analysis_data['search_keywords'])}
- **Total Products Found**: {len(competitors)} solutions
- **Data Source**: Product Hunt
- **Analysis Date**: {timestamp}
- **Processing Time**: {analysis_data['processing_time_seconds']:.2f} seconds

## ğŸ† Top Products by User Base

| Rank | Product | Users | Revenue Est. | Rating | Launch Date |
|------|---------|-------|--------------|--------|-------------|
"""
    
    # Add top competitors table
    for i, comp in enumerate(competitors[:10], 1):
        users = f"{comp.estimated_users:,}" if comp.estimated_users else "N/A"
        revenue = comp.estimated_revenue or "Early stage"
        rating = f"{comp.average_rating}â˜… ({comp.review_count} reviews)" if comp.average_rating and comp.review_count else "N/A"
        launch_date = comp.launch_date or "N/A"
        
        report += f"| {i} | **{comp.name}** | {users} | {revenue} | {rating} | {launch_date} |\n"
    
    # Founder/CEO information if available
    founders_info = [comp for comp in competitors if comp.founder_ceo]
    if founders_info:
        report += f"""
## ğŸ‘¥ Founder/CEO Information

| Product | Founder/CEO | Launch Date | Company Stage |
|---------|-------------|-------------|---------------|
"""
        for comp in founders_info[:10]:
            stage = "Growth stage" if comp.estimated_users and comp.estimated_users > 5000 else "Early stage"
            report += f"| **{comp.name}** | {comp.founder_ceo} | {comp.launch_date or 'N/A'} | {stage} |\n"
    
    # Review analysis if available
    rated_products = [comp for comp in competitors if comp.average_rating]
    if rated_products:
        report += f"""
## â­ Review Analysis

### Highest Rated Products:
"""
        # Sort by rating
        rated_products.sort(key=lambda x: x.average_rating or 0, reverse=True)
        for i, comp in enumerate(rated_products[:5], 1):
            rating = f"{comp.average_rating}â˜… ({comp.review_count} reviews)" if comp.review_count else f"{comp.average_rating}â˜…"
            report += f"{i}. **{comp.name}** - {rating}\n"
        
        # Add helpful reviews if available
        helpful_reviews = [comp for comp in competitors if comp.most_helpful_review]
        if helpful_reviews:
            report += f"""
### Most Helpful Reviews:

"""
            for comp in helpful_reviews[:5]:
                report += f"""**{comp.name}**:
> "{comp.most_helpful_review}"

"""
    
    # Market insights
    report += f"""
## ğŸ“ˆ Market Insights

### Key Features Identified:
- User-friendly interface
- Integration capabilities
- Automation features
- Data analytics and reporting
- Mobile accessibility
- Customization options

### Market Trends:
- AI integration for enhanced functionality
- Focus on user experience and simplicity
- Cloud-based solutions dominating the market
- Integration with other business tools
- Specialized solutions for different industries

## ğŸ’° Pricing Models

Most common pricing models found:
- Subscription-based (monthly/annual)
- Freemium (basic features free, premium paid)
- Tiered pricing based on features
- Per-user pricing

## ğŸ¯ Market Opportunities

1. **User Experience**: Simplified, intuitive interfaces
2. **Integration Ecosystem**: Better integration with popular tools
3. **AI-Powered**: Intelligent automation and insights
4. **Industry-Specific**: Solutions tailored for specific industries
5. **Mobile-First**: Solutions designed primarily for mobile use

---
*Data extracted from Product Hunt using enhanced scraping technology*
*Analysis includes launch dates, founder information, user reviews, and market positioning*
"""
    
    return report


async def test_google_scraper(query: str):
    """Test Google scraper with the given query."""
    print("=" * 80)
    print(f"ğŸš€ GOOGLE SCRAPER TEST: {query}")
    print("=" * 80)
    
    # Initialize the scraper
    scraper = GoogleScraper()
    
    # Generate keywords from query
    keywords = query.split()
    idea_text = f"A {query} platform for businesses"
    
    print(f"ğŸ“Š Search Keywords: {', '.join(keywords)}")
    print(f"ğŸ’¡ Business Idea: {idea_text}")
    print(f"ğŸ“… Analysis Date: {datetime.now().strftime('%B %d, %Y')}")
    print("\nStarting Google scraping...")
    
    try:
        # Execute the scraping
        start_time = datetime.now()
        result = await scraper.scrape(keywords, idea_text)
        end_time = datetime.now()
        
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… Scraping completed in {processing_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result.status}")
        
        if result.error_message:
            print(f"âŒ Error: {result.error_message}")
        
        # Process competitor data
        competitors = result.competitors
        
        # Process feedback data
        feedback_data = result.feedback
        
        # Generate analysis data
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'search_keywords': keywords,
            'business_idea': idea_text,
            'total_competitors': len(competitors),
            'total_feedback': len(feedback_data),
            'data_source': 'Google Search',
            'processing_time_seconds': processing_time,
            'scraping_status': result.status.value,
            'competitors': [comp.__dict__ for comp in competitors],
            'feedback': [fb.__dict__ for fb in feedback_data],
            'metadata': result.metadata
        }
        
        # Create output directories
        output_dir = Path('output')
        csv_dir = output_dir / 'csv'
        
        # Ensure directories exist
        output_dir.mkdir(exist_ok=True)
        csv_dir.mkdir(exist_ok=True)
        
        # Save JSON data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = output_dir / f'google_{query.replace(" ", "_")}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        
        # Save CSV data for competitors
        if competitors:
            csv_file = csv_dir / f'google_{query.replace(" ", "_")}_{timestamp}.csv'
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                fieldnames = [
                    'name', 'description', 'website', 'estimated_users', 'estimated_revenue', 
                    'pricing_model', 'confidence_score', 'source', 'source_url', 'launch_date',
                    'founder_ceo', 'review_count', 'average_rating', 'most_helpful_review'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for comp in competitors:
                    writer.writerow({
                        'name': comp.name,
                        'description': comp.description or 'N/A',
                        'website': comp.website or 'N/A',
                        'estimated_users': comp.estimated_users or 'N/A',
                        'estimated_revenue': comp.estimated_revenue or 'N/A',
                        'pricing_model': comp.pricing_model or 'N/A',
                        'confidence_score': comp.confidence_score,
                        'source': comp.source,
                        'source_url': comp.source_url or 'N/A',
                        'launch_date': comp.launch_date or 'N/A',
                        'founder_ceo': comp.founder_ceo or 'N/A',
                        'review_count': comp.review_count or 'N/A',
                        'average_rating': comp.average_rating or 'N/A',
                        'most_helpful_review': comp.most_helpful_review or 'N/A'
                    })
        
        # Generate markdown report
        if competitors:
            markdown_report = generate_google_markdown_report(query, analysis_data, competitors, feedback_data)
            md_file = output_dir / f'google_{query.replace(" ", "_")}_{timestamp}.md'
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(markdown_report)
        
        # Display results
        print(f"\nğŸ“ˆ ANALYSIS RESULTS")
        print(f"{'='*50}")
        print(f"ğŸ¢ Total Competitors Found: {len(competitors)}")
        print(f"ğŸ’¬ Feedback Items: {len(feedback_data)}")
        print(f"â±ï¸ Processing Time: {processing_time:.2f} seconds")
        
        if result.metadata:
            print(f"ğŸ” Keywords Searched: {result.metadata.get('keywords_searched', 'N/A')}")
            print(f"ğŸ“Š Search Queries: {result.metadata.get('search_queries', 'N/A')}")
        
        if competitors:
            print(f"\nğŸ† TOP COMPETITORS FROM GOOGLE")
            print(f"{'='*50}")
            for i, comp in enumerate(competitors[:5], 1):
                print(f"{i}. {comp.name}")
                print(f"   ğŸ“ Description: {comp.description[:100]}..." if comp.description and len(comp.description) > 100 else f"   ğŸ“ Description: {comp.description or 'N/A'}")
                print(f"   ğŸŒ Website: {comp.website or 'N/A'}")
                print(f"   ğŸ’° Revenue: {comp.estimated_revenue or 'N/A'}")
                print(f"   ğŸ·ï¸ Pricing: {comp.pricing_model or 'N/A'}")
                print(f"   ğŸ” Confidence: {comp.confidence_score:.2f}")
                
                # Display reviews/comments if available
                if comp.most_helpful_review:
                    print(f"   ğŸ’¬ Top Review: \"{comp.most_helpful_review[:100]}...\"" if len(comp.most_helpful_review) > 100 else f"   ğŸ’¬ Top Review: \"{comp.most_helpful_review}\"")
                
                # Display ratings if available
                if comp.average_rating:
                    print(f"   â­ Rating: {comp.average_rating} ({comp.review_count} reviews)")
                
                # Display launch date and founder if available
                if comp.launch_date:
                    print(f"   ğŸš€ Launched: {comp.launch_date}")
                if comp.founder_ceo:
                    print(f"   ğŸ‘¤ Founder/CEO: {comp.founder_ceo}")
                
                print()
        else:
            print("\nâŒ No competitors found")
        
        print(f"ğŸ“ FILES GENERATED:")
        print(f"   ğŸ“Š JSON Analysis: {json_file}")
        if competitors:
            print(f"   ğŸ“‹ CSV Data: {csv_file}")
            print(f"   ğŸ“ Markdown Report: {md_file}")
        
        return analysis_data
        
    except Exception as e:
        logger.error(f"Analysis failed: {str(e)}")
        print(f"âŒ Analysis failed: {str(e)}")
        return None


async def test_reddit_scraper(query: str):
    """Test Reddit scraper with the given query."""
    print("=" * 80)
    print(f"ğŸš€ REDDIT SCRAPER TEST: {query}")
    print("=" * 80)
    
    # Initialize the scraper
    scraper = RedditScraper()
    
    # Generate keywords from query
    keywords = query.split()
    idea_text = f"A {query} platform for businesses"
    
    print(f"ğŸ“Š Search Keywords: {', '.join(keywords)}")
    print(f"ğŸ’¡ Business Idea: {idea_text}")
    print(f"ğŸ“… Analysis Date: {datetime.now().strftime('%B %d, %Y')}")
    print("\nStarting Reddit scraping...")
    
    try:
        # Execute the scraping
        start_time = datetime.now()
        result = await scraper.scrape(keywords, idea_text)
        end_time = datetime.now()
        
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… Scraping completed in {processing_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result.status}")
        
        if result.error_message:
            print(f"âŒ Error: {result.error_message}")
        
        # Process competitor data
        competitors = result.competitors
        
        # Process feedback data
        feedback_data = result.feedback
        
        # Generate analysis data
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'search_keywords': keywords,
            'business_idea': idea_text,
            'total_competitors': len(competitors),
            'total_feedback': len(feedback_data),
            'data_source': 'Reddit',
            'processing_time_seconds': processing_time,
            'scraping_status': result.status.value,
            'competitors': [comp.__dict__ for comp in competitors],
            'feedback': [fb.__dict__ for fb in feedback_data],
            'metadata': result.metadata
        }
        
        # Create output directories
        output_dir = Path('output')
        csv_dir = output_dir / 'csv'
        
        # Ensure directories exist
        output_dir.mkdir(exist_ok=True)
        csv_dir.mkdir(exist_ok=True)
        
        # Save JSON data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = output_dir / f'reddit_{query.replace(" ", "_")}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        
        # Display results
        print(f"\nğŸ“ˆ ANALYSIS RESULTS")
        print(f"{'='*50}")
        print(f"ğŸ¢ Total Competitors Found: {len(competitors)}")
        print(f"ğŸ’¬ Feedback Items: {len(feedback_data)}")
        print(f"â±ï¸ Processing Time: {processing_time:.2f} seconds")
        
        if result.metadata:
            print(f"ğŸ” Keywords Searched: {result.metadata.get('keywords_searched', 'N/A')}")
            print(f"ğŸ“Š Subreddits: {result.metadata.get('subreddits_searched', 'N/A')}")
        
        if competitors:
            print(f"\nğŸ† TOP COMPETITORS FROM REDDIT")
            print(f"{'='*50}")
            for i, comp in enumerate(competitors[:5], 1):
                print(f"{i}. {comp.name}")
                print(f"   ğŸ“ Description: {comp.description or 'N/A'}")
                print(f"   ğŸ” Confidence: {comp.confidence_score:.2f}")
                print()
        else:
            print("\nâŒ No competitors found")
        
        if feedback_data:
            print(f"\nğŸ’¬ TOP FEEDBACK FROM REDDIT")
            print(f"{'='*50}")
            for i, fb in enumerate(feedback_data[:3], 1):
                sentiment_emoji = "ğŸ˜ƒ" if fb.sentiment == "positive" else "ğŸ˜" if fb.sentiment == "neutral" else "ğŸ˜"
                print(f"{i}. {sentiment_emoji} {fb.text[:100]}...")
                print(f"   Sentiment: {fb.sentiment} ({fb.sentiment_score:.2f})")
                print()
        
        print(f"ğŸ“ FILES GENERATED:")
        print(f"   ğŸ“Š JSON Analysis: {json_file}")
        
        return analysis_data
        
    except Exception as e:
        logger.error(f"Analysis failed: {str(e)}")
        print(f"âŒ Analysis failed: {str(e)}")
        return None


async def test_google_play_store_scraper(query: str):
    """Test Google Play Store scraper with the given query (Enhanced Mobile-First Version)."""
    print("=" * 80)
    print(f"ğŸš€ GOOGLE PLAY STORE SCRAPER TEST (MOBILE-OPTIMIZED): {query}")
    print("=" * 80)
    
    # Initialize the scraper
    scraper = GooglePlayStoreScraper()
    
    # Validate scraper configuration
    if not scraper.validate_config():
        print("âŒ Scraper configuration validation failed")
        return None
    
    # Generate keywords from query
    keywords = query.split()
    idea_text = f"A {query} app for mobile users"
    
    print(f"ğŸ“Š Search Keywords: {', '.join(keywords)}")
    print(f"ğŸ’¡ App Idea: {idea_text}")
    print(f"ğŸ“… Analysis Date: {datetime.now().strftime('%B %d, %Y')}")
    print(f"ğŸ”§ Scraper Version: Google Play Scraper API")
    print(f"ğŸ“± Max Results per Query: {scraper.max_results_per_query}")
    print(f"â±ï¸ Delay Range: {scraper.delay_between_requests[0]}-{scraper.delay_between_requests[1]} seconds")
    print(f"ğŸ¯ Max Queries: {scraper.max_queries}")
    print(f"ğŸŒ Supported Countries: {scraper.supported_countries}")
    print("\nStarting Google Play Store scraping...")
    
    try:
        # Execute the scraping
        start_time = datetime.now()
        result = await scraper.scrape(keywords, idea_text)
        end_time = datetime.now()
        
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… Scraping completed in {processing_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result.status}")
        
        if result.error_message:
            print(f"âŒ Error: {result.error_message}")
        
        # Process competitor data
        competitors = result.competitors
        feedback_data = result.feedback
        
        # Generate analysis data
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'search_keywords': keywords,
            'app_idea': idea_text,
            'total_competitors': len(competitors),
            'total_feedback': len(feedback_data),
            'data_source': 'Google Play Store (Mobile-Optimized)',
            'processing_time_seconds': processing_time,
            'scraping_status': result.status.value,
            'competitors': [comp.__dict__ for comp in competitors],
            'feedback': [fb.__dict__ for fb in feedback_data],
            'metadata': result.metadata,
            'scraper_config': {
                'max_results_per_query': scraper.max_results_per_query,
                'delay_range': scraper.delay_between_requests,
                'max_queries': scraper.max_queries,
                'api_based': True
            }
        }
        
        # Create output directories
        output_dir = Path('output')
        csv_dir = output_dir / 'csv'
        
        # Ensure directories exist
        output_dir.mkdir(exist_ok=True)
        csv_dir.mkdir(exist_ok=True)
        
        # Save JSON data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = output_dir / f'google_play_store_mobile_{query.replace(" ", "_")}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        
        # Save CSV data for competitors
        if competitors:
            csv_file = csv_dir / f'google_play_store_mobile_{query.replace(" ", "_")}_{timestamp}.csv'
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                fieldnames = [
                    'name', 'description', 'website', 'estimated_users', 'estimated_revenue', 
                    'pricing_model', 'confidence_score', 'source', 'source_url', 'launch_date',
                    'founder_ceo', 'review_count', 'average_rating', 'package_name'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for comp in competitors:
                    # Extract package name from source URL if available
                    package_name = 'N/A'
                    if comp.source_url and 'id=' in comp.source_url:
                        package_name = comp.source_url.split('id=')[1].split('&')[0]
                    
                    writer.writerow({
                        'name': comp.name,
                        'description': comp.description or 'N/A',
                        'website': comp.website or 'N/A',
                        'estimated_users': comp.estimated_users or 'N/A',
                        'estimated_revenue': comp.estimated_revenue or 'N/A',
                        'pricing_model': comp.pricing_model or 'N/A',
                        'confidence_score': comp.confidence_score,
                        'source': comp.source,
                        'source_url': comp.source_url or 'N/A',
                        'launch_date': comp.launch_date or 'N/A',
                        'founder_ceo': comp.founder_ceo or 'N/A',
                        'review_count': comp.review_count or 'N/A',
                        'average_rating': comp.average_rating or 'N/A',
                        'package_name': package_name
                    })
        
        # Display detailed results
        print(f"\nğŸ“ˆ DETAILED ANALYSIS RESULTS")
        print(f"{'='*60}")
        print(f"ğŸ“± Total Android Apps Found: {len(competitors)}")
        print(f"ğŸ’¬ Reviews/Feedback Items: {len(feedback_data)}")
        print(f"â±ï¸ Processing Time: {processing_time:.2f} seconds")
        print(f"ğŸ”„ API Calls Made: {result.metadata.get('api_calls_made', 'N/A')}")
        print(f"âœ… Successful Queries: {result.metadata.get('successful_queries', 'N/A')}")
        print(f"âŒ Failed Queries: {result.metadata.get('failed_queries', 'N/A')}")
        
        if result.metadata:
            print(f"ğŸ” Keywords Searched: {result.metadata.get('keywords_searched', 'N/A')}")
            print(f"ğŸ“Š Search Queries: {result.metadata.get('search_queries', 'N/A')}")
            print(f"ğŸ“± Apps Found: {result.metadata.get('apps_found', 'N/A')}")
            print(f"ğŸ“ Reviews Extracted: {result.metadata.get('reviews_extracted', 'N/A')}")
            print(f"ğŸ“ˆ Success Rate: {result.metadata.get('success_rate', 0):.1%}")
            print(f"ğŸ”§ Scraping Method: {result.metadata.get('scraping_method', 'N/A')}")
        
        if competitors:
            print(f"\nğŸ† TOP ANDROID APPS FROM GOOGLE PLAY STORE")
            print(f"{'='*70}")
            for i, comp in enumerate(competitors[:5], 1):
                print(f"{i}. ğŸ“± {comp.name}")
                print(f"   ğŸ‘¨â€ğŸ’» Developer: {comp.founder_ceo or 'N/A'}")
                print(f"   â­ Rating: {comp.average_rating or 'N/A'} ({comp.review_count or 'N/A'} reviews)")
                print(f"   ğŸ“± Installs: {comp.estimated_users or 'N/A'}")
                print(f"   ğŸ’° Pricing: {comp.pricing_model or 'N/A'}")
                print(f"   ğŸŒ Website: {comp.website or 'N/A'}")
                print(f"   ğŸ“ Description: {comp.description[:100]}..." if comp.description and len(comp.description) > 100 else f"   ğŸ“ Description: {comp.description or 'N/A'}")
                print(f"   ğŸ” Confidence: {comp.confidence_score:.2f}")
                
                # Extract and display package name
                if comp.source_url and 'id=' in comp.source_url:
                    package_name = comp.source_url.split('id=')[1].split('&')[0]
                    print(f"   ğŸ“¦ Package: {package_name}")
                
                print()
        else:
            print("\nâŒ No Android apps found")
            print("ğŸ’¡ This could be due to:")
            print("   - Bot detection by Google Play Store")
            print("   - Network connectivity issues")
            print("   - Changes in Play Store DOM structure")
            print("   - Rate limiting or IP blocking")
        
        if feedback_data:
            print(f"\nğŸ’¬ TOP FEEDBACK FROM GOOGLE PLAY STORE")
            print(f"{'='*60}")
            for i, fb in enumerate(feedback_data[:3], 1):
                print(f"{i}. {fb.text[:150]}...")
                app_name = 'N/A'
                if fb.author_info:
                    app_name = fb.author_info.get('app_name', 'N/A')
                    feedback_type = fb.author_info.get('type', 'review')
                    print(f"   ğŸ“± App: {app_name} ({feedback_type})")
                    if fb.author_info.get('rating'):
                        print(f"   â­ Rating: {fb.author_info.get('rating')}")
                print()
        
        # Performance analysis
        if result.metadata:
            print(f"\nâš¡ PERFORMANCE ANALYSIS")
            print(f"{'='*50}")
            queries_per_second = result.metadata.get('successful_queries', 0) / max(processing_time, 1)
            apps_per_second = len(competitors) / max(processing_time, 1)
            print(f"ğŸ“Š Queries per second: {queries_per_second:.2f}")
            print(f"ğŸ“± Apps extracted per second: {apps_per_second:.2f}")
            print(f"ğŸ”„ Average time per API call: {processing_time / max(result.metadata.get('api_calls_made', 1), 1):.2f}s")
            
            if result.metadata.get('failed_queries', 0) > 0:
                print(f"âš ï¸  API errors occurred on {result.metadata.get('failed_queries')} queries")
        
        print(f"\nğŸ“ FILES GENERATED:")
        print(f"   ğŸ“Š JSON Analysis: {json_file}")
        if competitors:
            print(f"   ğŸ“‹ CSV Data: {csv_file}")
        
        # Recommendations based on results
        print(f"\nğŸ’¡ RECOMMENDATIONS:")
        if len(competitors) == 0:
            print("   ğŸ”§ Try different or broader keywords")
            print("   â±ï¸  Check network connectivity")
            print("   ğŸŒ Verify Google Play Scraper API is accessible")
        elif len(competitors) < 3:
            print("   ğŸ“ˆ Results are limited - consider broader keywords")
            print("   ğŸ”„ Try running again with different search terms")
        else:
            print("   âœ… Good results obtained with API-based approach")
            print("   ğŸ“Š Data quality appears reliable for market analysis")
        
        return analysis_data
        
    except Exception as e:
        logger.error(f"Google Play Store analysis failed: {str(e)}")
        print(f"âŒ Google Play Store analysis failed: {str(e)}")
        print(f"\nğŸ”§ TROUBLESHOOTING TIPS:")
        print("   1. Check internet connection")
        print("   2. Verify Google Play Scraper API is accessible")
        print("   3. Try with simpler keywords")
        print("   4. Check if Google Play Store is accessible from your location")
        print("   5. Consider using different search terms")
        return None



async def test_app_store_scraper(query: str):
    """Test iOS App Store scraper with the given query."""
    print("=" * 80)
    print(f"ğŸš€ iOS APP STORE SCRAPER TEST: {query}")
    print("=" * 80)
    
    # Initialize the scraper
    scraper = AppStoreScraper()
    
    # Generate keywords from query
    keywords = query.split()
    idea_text = f"A {query} app for iOS users"
    
    print(f"ğŸ“Š Search Keywords: {', '.join(keywords)}")
    print(f"ğŸ’¡ App Idea: {idea_text}")
    print(f"ğŸ“… Analysis Date: {datetime.now().strftime('%B %d, %Y')}")
    print("\nStarting iOS App Store scraping...")
    
    try:
        # Execute the scraping
        start_time = datetime.now()
        result = await scraper.scrape(keywords, idea_text)
        end_time = datetime.now()
        
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… Scraping completed in {processing_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result.status}")
        
        if result.error_message:
            print(f"âŒ Error: {result.error_message}")
        
        # Process competitor data
        competitors = result.competitors
        feedback_data = result.feedback
        
        # Generate analysis data
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'search_keywords': keywords,
            'app_idea': idea_text,
            'total_competitors': len(competitors),
            'total_feedback': len(feedback_data),
            'data_source': 'iOS App Store',
            'processing_time_seconds': processing_time,
            'scraping_status': result.status.value,
            'competitors': [comp.__dict__ for comp in competitors],
            'feedback': [fb.__dict__ for fb in feedback_data],
            'metadata': result.metadata
        }
        
        # Create output directories
        output_dir = Path('output')
        csv_dir = output_dir / 'csv'
        
        # Ensure directories exist
        output_dir.mkdir(exist_ok=True)
        csv_dir.mkdir(exist_ok=True)
        
        # Save JSON data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = output_dir / f'ios_app_store_{query.replace(" ", "_")}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        
        # Save CSV data for competitors
        if competitors:
            csv_file = csv_dir / f'ios_app_store_{query.replace(" ", "_")}_{timestamp}.csv'
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                fieldnames = [
                    'name', 'description', 'website', 'estimated_users', 'estimated_revenue', 
                    'pricing_model', 'confidence_score', 'source', 'source_url', 'launch_date',
                    'founder_ceo', 'review_count', 'average_rating'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for comp in competitors:
                    writer.writerow({
                        'name': comp.name,
                        'description': comp.description or 'N/A',
                        'website': comp.website or 'N/A',
                        'estimated_users': comp.estimated_users or 'N/A',
                        'estimated_revenue': comp.estimated_revenue or 'N/A',
                        'pricing_model': comp.pricing_model or 'N/A',
                        'confidence_score': comp.confidence_score,
                        'source': comp.source,
                        'source_url': comp.source_url or 'N/A',
                        'launch_date': comp.launch_date or 'N/A',
                        'founder_ceo': comp.founder_ceo or 'N/A',
                        'review_count': comp.review_count or 'N/A',
                        'average_rating': comp.average_rating or 'N/A'
                    })
        
        # Display results
        print(f"\nğŸ“ˆ ANALYSIS RESULTS")
        print(f"{'='*50}")
        print(f"ğŸ Total iOS Apps Found: {len(competitors)}")
        print(f"ğŸ’¬ Reviews/Feedback Items: {len(feedback_data)}")
        print(f"â±ï¸ Processing Time: {processing_time:.2f} seconds")
        
        if result.metadata:
            print(f"ğŸ” Keywords Searched: {result.metadata.get('keywords_searched', 'N/A')}")
            print(f"ğŸ“Š Apps Found: {result.metadata.get('apps_found', 'N/A')}")
            print(f"ğŸ“ Reviews Extracted: {result.metadata.get('reviews_extracted', 'N/A')}")
        
        if competitors:
            print(f"\nğŸ† TOP iOS APPS FROM APP STORE")
            print(f"{'='*50}")
            for i, comp in enumerate(competitors[:5], 1):
                print(f"{i}. {comp.name}")
                print(f"   ğŸ‘¨â€ğŸ’» Developer: {comp.founder_ceo or 'N/A'}")
                print(f"   â­ Rating: {comp.average_rating or 'N/A'} ({comp.review_count or 'N/A'} reviews)")
                print(f"   ğŸ’° Pricing: {comp.pricing_model or 'N/A'}")
                print(f"   ğŸŒ Website: {comp.website or 'N/A'}")
                print(f"   ğŸ“… Released: {comp.launch_date or 'N/A'}")
                print(f"   ğŸ“ Description: {comp.description[:100]}..." if comp.description and len(comp.description) > 100 else f"   ğŸ“ Description: {comp.description or 'N/A'}")
                print(f"   ğŸ” Confidence: {comp.confidence_score:.2f}")
                print()
        else:
            print("\nâŒ No iOS apps found")
        
        if feedback_data:
            print(f"\nğŸ’¬ TOP REVIEWS FROM APP STORE")
            print(f"{'='*50}")
            for i, fb in enumerate(feedback_data[:3], 1):
                print(f"{i}. {fb.text[:150]}...")
                print(f"   ğŸ App: {fb.author_info.get('app_name', 'N/A') if fb.author_info else 'N/A'}")
                print()
        
        print(f"ğŸ“ FILES GENERATED:")
        print(f"   ğŸ“Š JSON Analysis: {json_file}")
        if competitors:
            print(f"   ğŸ“‹ CSV Data: {csv_file}")
        
        return analysis_data
        
    except Exception as e:
        logger.error(f"iOS App Store analysis failed: {str(e)}")
        print(f"âŒ iOS App Store analysis failed: {str(e)}")
        return None


async def test_microsoft_store_scraper(query: str):
    """Test Microsoft Store scraper with the given query."""
    print("=" * 80)
    print(f"ğŸš€ MICROSOFT STORE SCRAPER TEST: {query}")
    print("=" * 80)
    
    # Initialize the scraper
    scraper = MicrosoftStoreScraper()
    
    # Generate keywords from query
    keywords = query.split()
    idea_text = f"A {query} app for Windows users"
    
    print(f"ğŸ“Š Search Keywords: {', '.join(keywords)}")
    print(f"ğŸ’¡ App Idea: {idea_text}")
    print(f"ğŸ“… Analysis Date: {datetime.now().strftime('%B %d, %Y')}")
    print("\nStarting Microsoft Store scraping...")
    
    try:
        # Execute the scraping
        start_time = datetime.now()
        result = await scraper.scrape(keywords, idea_text)
        end_time = datetime.now()
        
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… Scraping completed in {processing_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result.status}")
        
        if result.error_message:
            print(f"âŒ Error: {result.error_message}")
        
        # Process competitor data
        competitors = result.competitors
        feedback_data = result.feedback
        
        # Generate analysis data
        analysis_data = {
            'timestamp': datetime.now().isoformat(),
            'search_keywords': keywords,
            'app_idea': idea_text,
            'total_competitors': len(competitors),
            'total_feedback': len(feedback_data),
            'data_source': 'Microsoft Store',
            'processing_time_seconds': processing_time,
            'scraping_status': result.status.value,
            'competitors': [comp.__dict__ for comp in competitors],
            'feedback': [fb.__dict__ for fb in feedback_data],
            'metadata': result.metadata
        }
        
        # Create output directories
        output_dir = Path('output')
        csv_dir = output_dir / 'csv'
        
        # Ensure directories exist
        output_dir.mkdir(exist_ok=True)
        csv_dir.mkdir(exist_ok=True)
        
        # Save JSON data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = output_dir / f'microsoft_store_{query.replace(" ", "_")}_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, default=str)
        
        # Save CSV data for competitors
        if competitors:
            csv_file = csv_dir / f'microsoft_store_{query.replace(" ", "_")}_{timestamp}.csv'
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                fieldnames = [
                    'name', 'description', 'website', 'estimated_users', 'estimated_revenue', 
                    'pricing_model', 'confidence_score', 'source', 'source_url', 'launch_date',
                    'founder_ceo', 'review_count', 'average_rating'
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for comp in competitors:
                    writer.writerow({
                        'name': comp.name,
                        'description': comp.description or 'N/A',
                        'website': comp.website or 'N/A',
                        'estimated_users': comp.estimated_users or 'N/A',
                        'estimated_revenue': comp.estimated_revenue or 'N/A',
                        'pricing_model': comp.pricing_model or 'N/A',
                        'confidence_score': comp.confidence_score,
                        'source': comp.source,
                        'source_url': comp.source_url or 'N/A',
                        'launch_date': comp.launch_date or 'N/A',
                        'founder_ceo': comp.founder_ceo or 'N/A',
                        'review_count': comp.review_count or 'N/A',
                        'average_rating': comp.average_rating or 'N/A'
                    })
        
        # Display results
        print(f"\nğŸ“ˆ ANALYSIS RESULTS")
        print(f"{'='*50}")
        print(f"ğŸªŸ Total Windows Apps Found: {len(competitors)}")
        print(f"ğŸ’¬ Reviews/Feedback Items: {len(feedback_data)}")
        print(f"â±ï¸ Processing Time: {processing_time:.2f} seconds")
        
        if result.metadata:
            print(f"ğŸ” Keywords Searched: {result.metadata.get('keywords_searched', 'N/A')}")
            print(f"ğŸ“Š Apps Found: {result.metadata.get('apps_found', 'N/A')}")
            print(f"ğŸ“ Reviews Extracted: {result.metadata.get('reviews_extracted', 'N/A')}")
        
        if competitors:
            print(f"\nğŸ† TOP WINDOWS APPS FROM MICROSOFT STORE")
            print(f"{'='*55}")
            for i, comp in enumerate(competitors[:5], 1):
                print(f"{i}. {comp.name}")
                print(f"   ğŸ‘¨â€ğŸ’» Developer: {comp.founder_ceo or 'N/A'}")
                print(f"   â­ Rating: {comp.average_rating or 'N/A'} ({comp.review_count or 'N/A'} reviews)")
                print(f"   ğŸ’° Pricing: {comp.pricing_model or 'N/A'}")
                print(f"   ğŸŒ Website: {comp.website or 'N/A'}")
                print(f"   ğŸ“… Released: {comp.launch_date or 'N/A'}")
                print(f"   ğŸ“ Description: {comp.description[:100]}..." if comp.description and len(comp.description) > 100 else f"   ğŸ“ Description: {comp.description or 'N/A'}")
                print(f"   ğŸ” Confidence: {comp.confidence_score:.2f}")
                print()
        else:
            print("\nâŒ No Windows apps found")
        
        if feedback_data:
            print(f"\nğŸ’¬ TOP REVIEWS FROM MICROSOFT STORE")
            print(f"{'='*50}")
            for i, fb in enumerate(feedback_data[:3], 1):
                print(f"{i}. {fb.text[:150]}...")
                print(f"   ğŸªŸ App: {fb.author_info.get('app_name', 'N/A') if fb.author_info else 'N/A'}")
                print()
        
        print(f"ğŸ“ FILES GENERATED:")
        print(f"   ğŸ“Š JSON Analysis: {json_file}")
        if competitors:
            print(f"   ğŸ“‹ CSV Data: {csv_file}")
        
        return analysis_data
        
    except Exception as e:
        logger.error(f"Microsoft Store analysis failed: {str(e)}")
        print(f"âŒ Microsoft Store analysis failed: {str(e)}")
        return None


def parse_arguments():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description='Test scrapers with specific queries')
    
    parser.add_argument('--product-hunt', action='store_true', help='Test Product Hunt scraper')
    parser.add_argument('--google', action='store_true', help='Test Google scraper')
    parser.add_argument('--reddit', action='store_true', help='Test Reddit scraper')
    parser.add_argument('--google-play', action='store_true', help='Test Google Play Store scraper (Mobile-Optimized)')
    parser.add_argument('--app-store', action='store_true', help='Test iOS App Store scraper')
    parser.add_argument('--microsoft-store', action='store_true', help='Test Microsoft Store scraper')
    parser.add_argument('--app-stores', action='store_true', help='Test all app store scrapers')
    parser.add_argument('--all', action='store_true', help='Test all scrapers')
    parser.add_argument('--sentiment-demo', action='store_true', help='Run comprehensive scraping + sentiment analysis demo')
    parser.add_argument('--query', type=str, default='productivity app', help='Search query to test')
    
    return parser.parse_args()


async def main():
    """Main entry point."""
    args = parse_arguments()
    
    # Check for sentiment demo first
    if args.sentiment_demo:
        await comprehensive_sentiment_demo(args.query)
        return
    
    if not (args.product_hunt or args.google or args.reddit or args.google_play or 
            args.app_store or args.microsoft_store or args.app_stores or args.all):
        print("ğŸš€ SCRAPER TESTING TOOL")
        print("=" * 50)
        print("Please specify at least one scraper to test:")
        print()
        print("ğŸ¯ COMPREHENSIVE DEMO:")
        print("  --sentiment-demo  Run comprehensive scraping + sentiment analysis demo")
        print()
        print("ğŸ“± APP STORE SCRAPERS:")
        print("  --google-play     Test Google Play Store scraper (Android - Mobile-Optimized)")
        print("  --app-store       Test iOS App Store scraper (iPhone/iPad)")
        print("  --microsoft-store Test Microsoft Store scraper (Windows)")
        print("  --app-stores      Test all app store scrapers")
        print()
        print("ğŸŒ WEB SCRAPERS:")
        print("  --product-hunt    Test Product Hunt scraper")
        print("  --google          Test Google search scraper")
        print("  --reddit          Test Reddit scraper")
        print()
        print("ğŸ”§ COMBINED OPTIONS:")
        print("  --all             Test all scrapers")
        print()
        print("ğŸ“ USAGE EXAMPLES:")
        print("  python test_scrapers.py --sentiment-demo --query 'productivity app'")
        print("  python test_scrapers.py --google-play --query 'fitness app'")
        print("  python test_scrapers.py --product-hunt --query 'productivity tool'")
        print("  python test_scrapers.py --app-stores --query 'business validation'")
        print("  python test_scrapers.py --all --query 'market research'")
        print()
        print("ğŸ’¡ RECOMMENDED: Start with the comprehensive demo:")
        print("  python test_scrapers.py --sentiment-demo --query 'your idea here'")
        return
    
    if args.all or args.product_hunt:
        await test_product_hunt_scraper(args.query)
    
    if args.all or args.google_play or args.app_stores:
        await test_google_play_store_scraper(args.query)


if __name__ == "__main__":
    asyncio.run(main())